{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"b5mWiul2qpIW"},"source":["## Mount the Drive, and Change to Google Drive Folder"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20292,"status":"ok","timestamp":1685703332650,"user":{"displayName":"Siwang Lam (Robin)","userId":"07119757220663994168"},"user_tz":-60},"id":"Tbthf1NQqCI_","outputId":"0523dfd5-044d-40e2-ba09-211cfed04eb1"},"outputs":[],"source":["# from google.colab import drive\n","# drive.mount('/content/drive', force_remount = True)\n","\n","# %cd /content/drive/MyDrive/MSc.-Dissertations/1/Files\n","# %ls"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"sdt1cPGxrIS9"},"source":["## Import Libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":4983,"status":"ok","timestamp":1685703337623,"user":{"displayName":"Siwang Lam (Robin)","userId":"07119757220663994168"},"user_tz":-60},"id":"kJuXqpDDXskx"},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from keras import models, layers, utils, losses\n","from keras.wrappers import scikit_learn\n","from keras.models import *\n","from keras.layers import *\n","from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n","from keras.utils import np_utils\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split, cross_val_score, KFold\n","from sklearn.pipeline import Pipeline\n","from sklearn.metrics import confusion_matrix\n","\n","import random\n","import pandas as pd\n","import numpy as np\n","from PIL import Image, ImageFilter\n","import matplotlib.pyplot as plt"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"14v7-4Ahre6v"},"source":["## Count the Number of Files, and Take Random Samples from the Image Files"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":393,"status":"ok","timestamp":1685703337958,"user":{"displayName":"Siwang Lam (Robin)","userId":"07119757220663994168"},"user_tz":-60},"id":"3490JUu9rPbw"},"outputs":[],"source":["# !ls street_view\n","# count how many files and write the filenames into a file\n","# !ls street_view -1 | wc -l \n","# !ls street_view/*.jpg > flist.txt\n","flist_old = list(pd.read_csv('flist.txt', header = None)[0])\n","flist = []\n","change_names = list(map(lambda x: flist.append(f'street_view/{x}'), flist_old))\n","\n","# Set seed so sample is reproducible \n","# random.seed(99)  # set this to an integer value!!!\n","# nsamp = 100\n","# flist_sub = random.sample(flist, nsamp)\n","# flist = flist_sub\n","\n","# print(flist)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Wkzy9pS2sATL"},"source":["## Overview of the `properties` Dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Read the `properties` dataset first, and make sure that `property type` is a categorical variable."]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":354},"executionInfo":{"elapsed":318,"status":"ok","timestamp":1685703338269,"user":{"displayName":"Siwang Lam (Robin)","userId":"07119757220663994168"},"user_tz":-60},"id":"YUl8gDe9jXof","outputId":"82aa5d04-bd88-4520-b79f-23c955888237"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>address</th>\n","      <th>propertyType</th>\n","      <th>bedrooms</th>\n","      <th>detailUrl</th>\n","      <th>location_lat</th>\n","      <th>location_lng</th>\n","      <th>property_id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>12, Gorsey Brigg, Dronfield Woodhouse, Dronfie...</td>\n","      <td>Terraced</td>\n","      <td>3.0</td>\n","      <td>https://www.rightmove.co.uk/house-prices/detai...</td>\n","      <td>53.29986</td>\n","      <td>-1.49446</td>\n","      <td>60d9dd15-c5a0-4d9c-a341-a1d47add49d5</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>5, Highgate Lane, Dronfield, Derbyshire S18 1UB</td>\n","      <td>Detached</td>\n","      <td>4.0</td>\n","      <td>https://www.rightmove.co.uk/house-prices/detai...</td>\n","      <td>53.29135</td>\n","      <td>-1.45975</td>\n","      <td>4a586e80-181a-4b82-b5c3-2d789436bb14</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>125, Gosforth Lane, Dronfield, Derbyshire S18 1RB</td>\n","      <td>Detached</td>\n","      <td>3.0</td>\n","      <td>https://www.rightmove.co.uk/house-prices/detai...</td>\n","      <td>53.29763</td>\n","      <td>-1.47573</td>\n","      <td>93680b6c-237e-44d3-8f40-959a14b80cad</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>80, Shakespeare Crescent, Dronfield, Derbyshir...</td>\n","      <td>Detached</td>\n","      <td>3.0</td>\n","      <td>https://www.rightmove.co.uk/house-prices/detai...</td>\n","      <td>53.29259</td>\n","      <td>-1.45644</td>\n","      <td>5d49758b-f148-4d06-bbae-3eb23f5c68fb</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>21, Gainsborough Road, Dronfield, Derbyshire S...</td>\n","      <td>Detached</td>\n","      <td>NaN</td>\n","      <td>https://www.rightmove.co.uk/house-prices/detai...</td>\n","      <td>53.29740</td>\n","      <td>-1.48503</td>\n","      <td>4645f5eb-de7c-474f-8d7e-b59fa8c55f19</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0                                            address propertyType  \\\n","0           0  12, Gorsey Brigg, Dronfield Woodhouse, Dronfie...     Terraced   \n","1           0    5, Highgate Lane, Dronfield, Derbyshire S18 1UB     Detached   \n","2           0  125, Gosforth Lane, Dronfield, Derbyshire S18 1RB     Detached   \n","3           0  80, Shakespeare Crescent, Dronfield, Derbyshir...     Detached   \n","4           0  21, Gainsborough Road, Dronfield, Derbyshire S...     Detached   \n","\n","   bedrooms                                          detailUrl  location_lat  \\\n","0       3.0  https://www.rightmove.co.uk/house-prices/detai...      53.29986   \n","1       4.0  https://www.rightmove.co.uk/house-prices/detai...      53.29135   \n","2       3.0  https://www.rightmove.co.uk/house-prices/detai...      53.29763   \n","3       3.0  https://www.rightmove.co.uk/house-prices/detai...      53.29259   \n","4       NaN  https://www.rightmove.co.uk/house-prices/detai...      53.29740   \n","\n","   location_lng                           property_id  \n","0      -1.49446  60d9dd15-c5a0-4d9c-a341-a1d47add49d5  \n","1      -1.45975  4a586e80-181a-4b82-b5c3-2d789436bb14  \n","2      -1.47573  93680b6c-237e-44d3-8f40-959a14b80cad  \n","3      -1.45644  5d49758b-f148-4d06-bbae-3eb23f5c68fb  \n","4      -1.48503  4645f5eb-de7c-474f-8d7e-b59fa8c55f19  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["properties = pd.read_csv('properties.csv')\n","properties_juny12 = pd.read_csv('properties_juny12.csv')\n","properties_full = pd.concat([properties, properties_juny12])\n","properties = properties_full\n","properties.propertyType = properties.propertyType.astype('category')\n","properties.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Basic information of the dataset is shown as follows."]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":404,"status":"ok","timestamp":1685703338661,"user":{"displayName":"Siwang Lam (Robin)","userId":"07119757220663994168"},"user_tz":-60},"id":"zFHpWHe8ndW6","outputId":"a9a60486-f56c-41ca-d447-5e36dc1dcb3e"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","Int64Index: 37402 entries, 0 to 19851\n","Data columns (total 8 columns):\n"," #   Column        Non-Null Count  Dtype   \n","---  ------        --------------  -----   \n"," 0   Unnamed: 0    37402 non-null  int64   \n"," 1   address       37402 non-null  object  \n"," 2   propertyType  37402 non-null  category\n"," 3   bedrooms      24486 non-null  float64 \n"," 4   detailUrl     37402 non-null  object  \n"," 5   location_lat  37402 non-null  float64 \n"," 6   location_lng  37402 non-null  float64 \n"," 7   property_id   37402 non-null  object  \n","dtypes: category(1), float64(3), int64(1), object(3)\n","memory usage: 2.3+ MB\n"]}],"source":["properties.info()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Descriptive statistics of continuous variables are shown as follows."]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1685703338662,"user":{"displayName":"Siwang Lam (Robin)","userId":"07119757220663994168"},"user_tz":-60},"id":"8ser64Zzm7Km","outputId":"e959f663-769b-4db4-a546-9201e2fa3f63"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>bedrooms</th>\n","      <th>location_lat</th>\n","      <th>location_lng</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>37402.0</td>\n","      <td>24486.000000</td>\n","      <td>37402.000000</td>\n","      <td>37402.000000</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.0</td>\n","      <td>2.875194</td>\n","      <td>52.868188</td>\n","      <td>-2.089771</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.0</td>\n","      <td>0.993353</td>\n","      <td>1.874818</td>\n","      <td>1.502522</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>0.0</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>-7.557160</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.0</td>\n","      <td>2.000000</td>\n","      <td>51.408743</td>\n","      <td>-3.066690</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.0</td>\n","      <td>3.000000</td>\n","      <td>52.650855</td>\n","      <td>-2.187560</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.0</td>\n","      <td>3.000000</td>\n","      <td>53.775813</td>\n","      <td>-0.938630</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>0.0</td>\n","      <td>12.000000</td>\n","      <td>60.162730</td>\n","      <td>1.618240</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       Unnamed: 0      bedrooms  location_lat  location_lng\n","count     37402.0  24486.000000  37402.000000  37402.000000\n","mean          0.0      2.875194     52.868188     -2.089771\n","std           0.0      0.993353      1.874818      1.502522\n","min           0.0      0.000000      0.000000     -7.557160\n","25%           0.0      2.000000     51.408743     -3.066690\n","50%           0.0      3.000000     52.650855     -2.187560\n","75%           0.0      3.000000     53.775813     -0.938630\n","max           0.0     12.000000     60.162730      1.618240"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["properties.describe()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Frequencies of each level of the variable `property type` are obtained as follows."]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1685703338663,"user":{"displayName":"Siwang Lam (Robin)","userId":"07119757220663994168"},"user_tz":-60},"id":"mMG-qHT_8rSr","outputId":"b2ad2fb9-4014-450e-e954-215b4e64db2a"},"outputs":[{"data":{"text/plain":["Semi-Detached    8783\n","Terraced         8535\n","Detached         8369\n","Unknown          6647\n","Flat             5068\n","Name: propertyType, dtype: int64"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["properties.propertyType.value_counts()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XDqLkqImshZy"},"source":["## A Subset of the `properties` Dataset"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["As random samples of images have been obtained previously, a subset of the whole `properties` dataset could hence be formulated by selecting the rows of the whole `properties` dataset corresponding to the selected samples."]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1685703338664,"user":{"displayName":"Siwang Lam (Robin)","userId":"07119757220663994168"},"user_tz":-60},"id":"Nqmh9e2Mofpj"},"outputs":[],"source":["flist_id = list(map(lambda string: string[16 : -4], flist))\n","properties_sub = pd.DataFrame(properties.loc[properties['property_id'].isin(flist_id)])\n","properties_sub = properties_sub.drop_duplicates(['location_lat', 'location_lng'])\n","flist_id = list(properties_sub.property_id)\n","flist_new = []\n","change_names_new = list(map(lambda x: flist_new.append(f'street_view/gsv_{x}.jpg'), flist_id))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The original data should be splitted into training and testing sets, and the testing set contains 30% of the original data."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["flist_train, flist_test, propertyType_train, propertyType_test = train_test_split(\n","    flist_new, # image directories\n","    properties_sub.propertyType, # property types\n","    test_size = 0.3)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["Img_list_train = list(map(lambda x: np.asarray(Image.open(x).resize((32, 32), Image.LANCZOS)), flist_train))\n","Img_list_test = list(map(lambda x: np.asarray(Image.open(x).resize((32, 32), Image.LANCZOS)), flist_test))\n","Img_array_train = np.asarray(Img_list_train)\n","Img_array_test = np.asarray(Img_list_test)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["For categorical variables, one-hot encoder is introduced."]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":70,"status":"ok","timestamp":1685703485477,"user":{"displayName":"Siwang Lam (Robin)","userId":"07119757220663994168"},"user_tz":-60},"id":"_bFYcWMtuQJA"},"outputs":[],"source":["dummy_propertyType_train = pd.get_dummies(propertyType_train)\n","dummy_propertyType_test = pd.get_dummies(propertyType_test)\n","propertyType_test_fac = np.argmax(np.array(dummy_propertyType_test), axis = 1) "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CzlhhsNtuQJB"},"source":["## Multi-Class Classification Using Neural Network"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Multi-Layer Perceptron (MLP) model"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":26,"status":"ok","timestamp":1685703485479,"user":{"displayName":"Siwang Lam (Robin)","userId":"07119757220663994168"},"user_tz":-60},"id":"cVOv_GReuQJB"},"outputs":[],"source":["def mlp(output_dim):\n","\n","    '''\n","    Creates a multi-layer perceptron neural network model without hidden layers.\n","\n","    Parameter:\n","    output_dim (int): The number of output classes.\n","    \n","    Returns:\n","    A compiled Keras model.\n","    '''\n","\n","    model = Sequential()\n","    model.add(Rescaling(1. / 255))\n","    model.add(Flatten())\n","    model.add(Dense(128, activation = tf.nn.leaky_relu))\n","    model.add(Dense(output_dim, activation = tf.nn.softmax))\n","    loss = keras.losses.CategoricalCrossentropy(from_logits = False)\n","    weights = np.array(len(propertyType_train) / propertyType_train.value_counts())\n","    loss.weighted = weights\n","    model.compile(loss = loss, optimizer = 'adam', metrics = ['accuracy'])\n","    return model"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6071,"status":"ok","timestamp":1685703491525,"user":{"displayName":"Siwang Lam (Robin)","userId":"07119757220663994168"},"user_tz":-60},"id":"zz6qw9ZvuQJB","outputId":"533c854e-87ca-4e7e-c441-1391c50bff64"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/32\n","43/43 [==============================] - 2s 23ms/step - loss: 2.1795 - accuracy: 0.2625\n","Epoch 2/32\n","43/43 [==============================] - 1s 24ms/step - loss: 1.5021 - accuracy: 0.3308\n","Epoch 3/32\n","43/43 [==============================] - 1s 26ms/step - loss: 1.4937 - accuracy: 0.3328\n","Epoch 4/32\n","43/43 [==============================] - 1s 33ms/step - loss: 1.4951 - accuracy: 0.3314\n","Epoch 5/32\n","43/43 [==============================] - 2s 34ms/step - loss: 1.4774 - accuracy: 0.3486\n","Epoch 6/32\n","43/43 [==============================] - 1s 28ms/step - loss: 1.4863 - accuracy: 0.3386\n","Epoch 7/32\n","43/43 [==============================] - 1s 27ms/step - loss: 1.4817 - accuracy: 0.3456\n","Epoch 8/32\n","43/43 [==============================] - 1s 31ms/step - loss: 1.4599 - accuracy: 0.3638\n","Epoch 9/32\n","43/43 [==============================] - 1s 26ms/step - loss: 1.4615 - accuracy: 0.3612\n","Epoch 10/32\n","43/43 [==============================] - 1s 26ms/step - loss: 1.4467 - accuracy: 0.3731\n","Epoch 11/32\n","43/43 [==============================] - 1s 29ms/step - loss: 1.4430 - accuracy: 0.3686\n","Epoch 12/32\n","43/43 [==============================] - 1s 27ms/step - loss: 1.4442 - accuracy: 0.3659\n","Epoch 13/32\n","43/43 [==============================] - 1s 27ms/step - loss: 1.4374 - accuracy: 0.3789\n","Epoch 14/32\n","43/43 [==============================] - 1s 29ms/step - loss: 1.4412 - accuracy: 0.3773\n","Epoch 15/32\n","43/43 [==============================] - 1s 25ms/step - loss: 1.4339 - accuracy: 0.3801\n","Epoch 16/32\n","43/43 [==============================] - 1s 29ms/step - loss: 1.4403 - accuracy: 0.3720\n","Epoch 17/32\n","43/43 [==============================] - 1s 29ms/step - loss: 1.4412 - accuracy: 0.3731\n","Epoch 18/32\n","43/43 [==============================] - 1s 28ms/step - loss: 1.4389 - accuracy: 0.3740\n","Epoch 19/32\n","43/43 [==============================] - 1s 26ms/step - loss: 1.4498 - accuracy: 0.3787\n","Epoch 20/32\n","43/43 [==============================] - 2s 35ms/step - loss: 1.4346 - accuracy: 0.3813\n","Epoch 21/32\n","43/43 [==============================] - 1s 29ms/step - loss: 1.4390 - accuracy: 0.3754\n","Epoch 22/32\n","43/43 [==============================] - 1s 25ms/step - loss: 1.4174 - accuracy: 0.3811\n","Epoch 23/32\n","43/43 [==============================] - 1s 27ms/step - loss: 1.4223 - accuracy: 0.3888\n","Epoch 24/32\n","43/43 [==============================] - 1s 26ms/step - loss: 1.4384 - accuracy: 0.3735\n","Epoch 25/32\n","43/43 [==============================] - 1s 24ms/step - loss: 1.4138 - accuracy: 0.3959\n","Epoch 26/32\n","43/43 [==============================] - 1s 25ms/step - loss: 1.4289 - accuracy: 0.3806\n","Epoch 27/32\n","43/43 [==============================] - 1s 28ms/step - loss: 1.4140 - accuracy: 0.3913\n","Epoch 28/32\n","43/43 [==============================] - 1s 25ms/step - loss: 1.4185 - accuracy: 0.3854\n","Epoch 29/32\n","43/43 [==============================] - 1s 26ms/step - loss: 1.4163 - accuracy: 0.3905\n","Epoch 30/32\n","43/43 [==============================] - 1s 28ms/step - loss: 1.4197 - accuracy: 0.3853\n","Epoch 31/32\n","43/43 [==============================] - 1s 26ms/step - loss: 1.4210 - accuracy: 0.3838\n","Epoch 32/32\n","43/43 [==============================] - 1s 25ms/step - loss: 1.4174 - accuracy: 0.3875\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x237d9e800d0>"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["MLP = mlp(dummy_propertyType_train.shape[1])\n","MLP.fit(ImageDataGenerator().flow(Img_array_train, dummy_propertyType_train, batch_size = 256),\n","        epochs = 32, batch_size = 64)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["146/146 [==============================] - 0s 2ms/step\n","146/146 [==============================] - 1s 2ms/step - loss: 1.5130 - accuracy: 0.3207\n"]},{"data":{"text/plain":["[1.5130411386489868, 0.32070598006248474]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["propertyType_test_pred_MLP = np.argmax(MLP.predict(Img_array_test), axis = 1) \n","MLP.evaluate(Img_array_test, dummy_propertyType_test)"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["array([[643, 113,  49, 155,  10],\n","       [195, 297,  25, 139,  11],\n","       [696, 184, 111, 271,  14],\n","       [418, 283,  58, 417,  11],\n","       [260,  88,  36, 140,  22]], dtype=int64)"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["confusion_matrix(propertyType_test_fac, propertyType_test_pred_MLP)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Convolutional Neural Network"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def cnn(output_dim):\n","\n","    '''\n","    Creates a convolutional neural network model without hidden layers.\n","\n","    Parameter:\n","    output_dim (int): The number of output classes.\n","    \n","    Returns:\n","    A compiled Keras model.\n","    '''\n","\n","    model = Sequential()\n","    model.add(Rescaling(1. / 255))\n","    model.add(Conv2D(4, 2, padding = 'same', activation = tf.nn.leaky_relu))\n","    model.add(MaxPooling2D())  \n","    model.add(Conv2D(8, 2, padding = 'same', activation = tf.nn.leaky_relu))\n","    model.add(MaxPooling2D()) \n","    model.add(Flatten())\n","    model.add(Dense(16, activation = tf.nn.leaky_relu))\n","    model.add(Dense(output_dim, activation = tf.nn.softmax))\n","    loss = keras.losses.CategoricalCrossentropy(from_logits = False)\n","    weights = np.array(len(propertyType_train) / propertyType_train.value_counts())\n","    loss.weighted = weights\n","    model.compile(loss = loss, optimizer = 'adam', metrics = ['accuracy'])\n","    return model"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/32\n","43/43 [==============================] - 4s 52ms/step - loss: 1.5498 - accuracy: 0.2801\n","Epoch 2/32\n","43/43 [==============================] - 2s 48ms/step - loss: 1.5306 - accuracy: 0.3010\n","Epoch 3/32\n","43/43 [==============================] - 2s 52ms/step - loss: 1.5128 - accuracy: 0.3152\n","Epoch 4/32\n","43/43 [==============================] - 2s 47ms/step - loss: 1.4990 - accuracy: 0.3222\n","Epoch 5/32\n","43/43 [==============================] - 2s 44ms/step - loss: 1.4831 - accuracy: 0.3347\n","Epoch 6/32\n","43/43 [==============================] - 2s 46ms/step - loss: 1.4739 - accuracy: 0.3442\n","Epoch 7/32\n","43/43 [==============================] - 2s 44ms/step - loss: 1.4598 - accuracy: 0.3512\n","Epoch 8/32\n","43/43 [==============================] - 2s 45ms/step - loss: 1.4535 - accuracy: 0.3542\n","Epoch 9/32\n","43/43 [==============================] - 2s 46ms/step - loss: 1.4487 - accuracy: 0.3647\n","Epoch 10/32\n","43/43 [==============================] - 2s 43ms/step - loss: 1.4437 - accuracy: 0.3656\n","Epoch 11/32\n","43/43 [==============================] - 2s 45ms/step - loss: 1.4367 - accuracy: 0.3733\n","Epoch 12/32\n","43/43 [==============================] - 2s 52ms/step - loss: 1.4339 - accuracy: 0.3748\n","Epoch 13/32\n","43/43 [==============================] - 2s 49ms/step - loss: 1.4322 - accuracy: 0.3783\n","Epoch 14/32\n","43/43 [==============================] - 2s 46ms/step - loss: 1.4275 - accuracy: 0.3796\n","Epoch 15/32\n","43/43 [==============================] - 2s 45ms/step - loss: 1.4265 - accuracy: 0.3766\n","Epoch 16/32\n","43/43 [==============================] - 2s 43ms/step - loss: 1.4214 - accuracy: 0.3830\n","Epoch 17/32\n","43/43 [==============================] - 2s 46ms/step - loss: 1.4181 - accuracy: 0.3848\n","Epoch 18/32\n","43/43 [==============================] - 2s 45ms/step - loss: 1.4193 - accuracy: 0.3884\n","Epoch 19/32\n","43/43 [==============================] - 2s 47ms/step - loss: 1.4137 - accuracy: 0.3905\n","Epoch 20/32\n","43/43 [==============================] - 2s 45ms/step - loss: 1.4103 - accuracy: 0.3895\n","Epoch 21/32\n","43/43 [==============================] - 2s 47ms/step - loss: 1.4082 - accuracy: 0.3887\n","Epoch 22/32\n","43/43 [==============================] - 2s 53ms/step - loss: 1.4066 - accuracy: 0.3939\n","Epoch 23/32\n","43/43 [==============================] - 2s 53ms/step - loss: 1.4044 - accuracy: 0.3945\n","Epoch 24/32\n","43/43 [==============================] - 2s 45ms/step - loss: 1.3998 - accuracy: 0.4000\n","Epoch 25/32\n","43/43 [==============================] - 2s 44ms/step - loss: 1.3976 - accuracy: 0.3998\n","Epoch 26/32\n","43/43 [==============================] - 2s 45ms/step - loss: 1.3958 - accuracy: 0.4016\n","Epoch 27/32\n","43/43 [==============================] - 2s 44ms/step - loss: 1.3880 - accuracy: 0.4058\n","Epoch 28/32\n","43/43 [==============================] - 2s 48ms/step - loss: 1.3845 - accuracy: 0.4102\n","Epoch 29/32\n","43/43 [==============================] - 2s 44ms/step - loss: 1.3778 - accuracy: 0.4137\n","Epoch 30/32\n","43/43 [==============================] - 2s 43ms/step - loss: 1.3773 - accuracy: 0.4164\n","Epoch 31/32\n","43/43 [==============================] - 2s 44ms/step - loss: 1.3741 - accuracy: 0.4134\n","Epoch 32/32\n","43/43 [==============================] - 2s 44ms/step - loss: 1.3719 - accuracy: 0.4183\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x237e241d390>"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["CNN = cnn(dummy_propertyType_train.shape[1])\n","CNN.fit(ImageDataGenerator().flow(Img_array_train, dummy_propertyType_train, batch_size = 256), \n","        epochs = 32, batch_size = 64)"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["146/146 [==============================] - 1s 7ms/step\n","146/146 [==============================] - 1s 6ms/step - loss: 1.4357 - accuracy: 0.3965\n"]},{"data":{"text/plain":["[1.4357295036315918, 0.3964700698852539]"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["propertyType_test_pred_CNN = np.argmax(CNN.predict(Img_array_test), axis = 1) \n","CNN.evaluate(Img_array_test, dummy_propertyType_test)"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[{"data":{"text/plain":["array([[343,  41, 387, 154,  45],\n","       [ 80, 186, 160, 212,  29],\n","       [245,  53, 684, 230,  64],\n","       [153,  74, 370, 529,  61],\n","       [ 94,  55, 190, 107, 100]], dtype=int64)"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["confusion_matrix(propertyType_test_fac, propertyType_test_pred_CNN)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["# import dill\n","# dill.dump_session('Presetting.pkl')\n","# # dill.load_session('Presetting.pkl')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":0}
